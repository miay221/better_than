import re
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from konlpy.tag import Komoran
import asyncio
from lime.lime_text import LimeTextExplainer

# Komoran í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
komoran = Komoran()

# Hugging Faceì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
MODEL_PATH = "miayyyyy/kcBERT_downstream_model"  # Hugging Face ì—…ë¡œë“œëœ ëª¨ë¸ ê²½ë¡œ
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)

# ì „ì²˜ë¦¬ í•¨ìˆ˜ ì •ì˜
def extract_lemma(text):
    """Komoranìœ¼ë¡œ í˜•íƒœì†Œ ì¶”ì¶œ"""
    morphs = komoran.morphs(text)
    return " ".join(morphs)

def preprocess_text(text):
    """ì‚¬ìš©ì í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    text = re.sub(r'[\U0001F600-\U0001F64F]', 'ğŸ˜Š', text)  # ê¸°ë³¸ ì´ëª¨í‹°ì½˜ì„ ì›ƒëŠ” ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ëŒ€ì²´
    text = re.sub(r'[\U0001F300-\U0001F5FF]', '', text)  # ê¸°íƒ€ ì‹¬ë³¼ ì´ëª¨í‹°ì½˜ ì œê±°
    text = re.sub(r'[\U0001F680-\U0001F6FF]', '', text)  # ìš´ì†¡ ì´ëª¨í‹°ì½˜ ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()             # ê³¼ë„í•œ ê³µë°± ì œê±°
    text = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]', '', text)  # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°± ì´ì™¸ ì œê±°
    text = extract_lemma(text)  # í˜•íƒœì†Œ ì¶”ì¶œ
    return text

# ê°ì • ì˜ˆì¸¡ í•¨ìˆ˜ ì •ì˜
async def predict_emotion_async(text):
    """ì‚¬ìš©ì í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ê°ì • ë¼ë²¨ ì˜ˆì¸¡"""
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    processed_text = preprocess_text(text)

    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(processed_text, return_tensors="pt", truncation=True, padding="max_length", max_length=128)

    await asyncio.sleep(0) # ë¹„ë™ê¸° ì‘ì—… ì˜ˆì•½

    # ëª¨ë¸ ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probabilities = torch.softmax(logits, dim=-1)
        predicted_class = torch.argmax(probabilities).item()  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ë°˜í™˜

    return {
        "processed_text": processed_text,
        "predicted_class": predicted_class,
        "probabilities": probabilities.tolist()  # ê° í´ë˜ìŠ¤ë³„ í™•ë¥  ë°˜í™˜
    }


async def generate_lime_explanation(text):
    explainer = LimeTextExplainer(class_names=["ë§¤ìš° ì¢‹ì§€ ì•ŠìŒ", "ì¢‹ì§€ ì•ŠìŒ", "ë³´í†µ", "ì¢‹ìŒ", "ë§¤ìš° ì¢‹ìŒ"])

    def predict_proba(texts):
        inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=150)
        with torch.no_grad():
            logits = model(**inputs).logits
            proba = torch.softmax(logits, dim=-1).cpu().numpy()
        return proba
    
    processed_text = preprocess_text(text)

    explanation = await asyncio.to_thread(
        explainer.explain_instance,
        processed_text,
        predict_proba,
        num_features=8
    )

    lime_html = explanation.as_html()
    return lime_html