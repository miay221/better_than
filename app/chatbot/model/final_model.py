import re
import pandas as pd
# from google.colab import drive
import torch
from torch.nn.functional import softmax  # 0~1 ì‚¬ì´ í™•ë¥ ë¡œ ë°˜í™˜
from lime.lime_text import LimeTextExplainer
from transformers import AutoModelForSequenceClassification, AutoTokenizer


# hugging face ì €ì¥ëœ ëª¨ë¸, í† í¬ë‚˜ì´ì € ë¡œë“œ
hf_model_path = "miayyyyy/downstream_kcBERT_emotionLabel"  # Hugging Face ëª¨ë¸ ê²½ë¡œ
tokenizer = AutoTokenizer.from_pretrained(hf_model_path)
model = AutoModelForSequenceClassification.from_pretrained(hf_model_path)

positive_words_file='./data/positive_words.csv'
negative_words_file='./data/negative_words.csv'

pos_words=pd.read_csv(positive_words_file)
neg_words=pd.read_csv(negative_words_file)


# ê¸ì •, ë¶€ì • ë‹¨ì–´ CSV íŒŒì¼ ë¡œë“œ
positive_words = pos_words['word'].tolist()
negative_words = neg_words['word'].tolist()

# ì…ë ¥ ì‹¤ì‹œê°„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_text(text):
    # ê¸ì • ë° ë¶€ì • ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì´ìš©í•´ ë„ì–´ì“°ê¸° ì²˜ë¦¬
    for word in positive_words + negative_words:
        if word in text and not f" {word} " in text:
            # ë¶™ì–´ìˆëŠ” ê²½ìš° ë„ì–´ì“°ê¸° ì¶”ê°€
            text = text.replace(word, f" {word} ")
    text = re.sub(r'[\U0001F600-\U0001F64F]', 'ğŸ˜Š', text)  # ê¸°ë³¸ ì´ëª¨í‹°ì½˜ì„ ì›ƒëŠ” ì´ëª¨í‹°ì½˜ìœ¼ë¡œ ëŒ€ì²´
    text = re.sub(r'[\U0001F300-\U0001F5FF]', '', text)  # ê¸°íƒ€ ì‹¬ë³¼ ì´ëª¨í‹°ì½˜ ì œê±°
    text = re.sub(r'[\U0001F680-\U0001F6FF]', '', text)  # ìš´ì†¡ ì´ëª¨í‹°ì½˜ ì œê±°
    text = re.sub(r'[\U0001F700-\U0001F77F]', '', text)  # ê¸°íƒ€ ì¶”ê°€ ì‹¬ë³¼ ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()     # 2. ê³¼ë„í•œ ê³µë°± ì œê±°
    text = re.sub(r'([ã„±-ã…ã…-ã…£ê°€-í£])\1{2,}', r'\1\1', text)   # 3. ììŒ/ëª¨ìŒ ë°˜ë³µ ì œ
    text = re.sub(r'[^ã„±-ã…ã…-ã…£ê°€-í£a-zA-Z0-9\s]', '', text)  # í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±ì„ ì œì™¸í•œ ë¬¸ì ì œê±°

    return text

# ì…ë ¥ í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìœ„í•œ í•¨ìˆ˜
def predict_proba(texts):
    inputs = tokenizer(texts, return_tensors="pt", truncation=True, padding=True, max_length=128)
    outputs = model(**inputs)
    logits = outputs.logits
    proba = torch.softmax(logits, dim=-1)
    return proba.detach().numpy()


# ì‚¬ìš©ì í…ìŠ¤íŠ¸ ë¬¸ë§¥ ë¼ë²¨ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_user_sentence(review_sentence):

  # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
  user_sentence = preprocess_text(review_sentence)

  # LIME ì„¤ì • ë° ì„¤ëª… ìƒì„±
  explainer = LimeTextExplainer(class_names=[0, 1, 2, 3, 4])  # ì˜ˆ: ê°ì • ë¼ë²¨ì´ 0~4ë¡œ ì„¤ì •ë¨
  explanation = explainer.explain_instance(user_sentence, predict_proba, num_features=4, num_samples=100)

  # í•˜ì´ë¼ì´íŒ…ëœ í…ìŠ¤íŠ¸ ì‹œê°í™”
  explanation.show_in_notebook(text=user_sentence)

  # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
  inputs = tokenizer(user_sentence, return_tensors="pt", truncation=True, padding=True, max_length=128)
  with torch.no_grad():
      outputs = model(**inputs)
      logits = outputs.logits

  predicted_probabilities = torch.softmax(logits, dim=-1)
  predicted_class = torch.argmax(predicted_probabilities, dim=-1).item()  # ì˜ˆì¸¡ëœ í´ë˜ìŠ¤ ì¸ë±ìŠ¤

  # ìµœì¢… ì˜ˆì¸¡ì´ ëë‚œ í›„ ë©”ëª¨ë¦¬ í•´ì œ
  del inputs
  torch.cuda.empty_cache()

  print('user_sentence:', user_sentence)
  print(f"ì˜ˆì¸¡ëœ ê°ì • ë¼ë²¨: {predicted_class}")

  return predicted_class


test_sentence = "ì•„ë¬´ë¦¬ê·¸ë˜ë„ ì´ê±´ ì¢€ ì•„ë‹Œë“¯"
predict_user_sentence(test_sentence)



