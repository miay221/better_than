{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('data/EXSA2002108040.csv')\n",
    "\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1= data[['expression_form', 'expression_score', 'expression_category', 'subject']]\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data_1['expression_form'] # 사용자 발화 내용\n",
    "labels = data_1['expression_score']  # 감정 라벨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# transformer 라이브러리에 맞춰서 Dataset 객체 정의하는 클래스\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "texts = data_1['expression_form'] # 사용자 발화 내용\n",
    "labels = data_1['expression_score']  # 감정 라벨\n",
    "\n",
    "# 학습 데이터셋과 검증 데이터셋으로 분리\n",
    "X_train, X_test, y_train, y_test=train_test_split(texts, labels, random_state=42)\n",
    "\n",
    "# 라벨 값 변환 (모델이 이해할 수 있는 숫자로 변환)\n",
    "label_mapping = {-2: 0, -1: 1, 0: 2, 1: 3, 2: 4}\n",
    "y_train = y_train.map(label_mapping)\n",
    "y_test = y_test.map(label_mapping)\n",
    "\n",
    "\n",
    "\n",
    "# 토크나이저 및 BERT 모델 로드\n",
    "num_classes = 5 # 감정 라벨이 5개로 분류되어 있음 (0,1,2,3,4 로 분류됨)\n",
    "tokenizer = BertTokenizer.from_pretrained('beomi/kcbert-base')\n",
    "model = BertForSequenceClassification.from_pretrained('beomi/kcbert-base', num_labels=num_classes)\n",
    "\n",
    "# 데이터 토크나이징\n",
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "\n",
    "# 토크나이징한 데이터를 Dataset으로 변환하기 (앞서 정의한 클래스 활용)\n",
    "train_dataset = EmotionDataset(train_encodings, y_train.tolist())   \n",
    "test_dataset = EmotionDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "\n",
    "\n",
    "# trainer 객체를 사용해서 학습시키기\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './results',\n",
    "    num_train_epochs=3, # 학습시킬 epoch 수 (epoch 이 많아지면 과적합될 수 있음)\n",
    "    per_device_train_batch_size=8,  # 학습시킬 배치 사이즈 (배치 사이즈 커지면 GPU 사용량이 많아짐)\n",
    "    per_device_eval_batch_size=8,  # 검증할때 배치 사이즈\n",
    "    warmup_steps=500, # 워밍업 스텝 수 (학습 초반 모델이 적응하도록 학습률을 천천히 증가시킴. 학습을 더 안정적으로 만듦)\n",
    "    weight_decay=0.01, # 가중치 감소 (과적합 방지를 위해 가중치 감소-정규화 시킴)\n",
    "    logging_dir='./logs',  # 로그 디렉터리 (학습 로그를 기록할 곳)\n",
    "    logging_steps=10, # 로그 기록할 스텝 수 지정\n",
    "    evaluation_strategy=\"steps\", # 학습 중 일정 스텝마다 평가하기\n",
    "    eval_steps=100,  # 평가 스텝 간격\n",
    "    load_best_model_at_end=True  # 학습 종료 시 최고의 모델 로드\n",
    ")\n",
    "\n",
    "# Trainer 객체 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./trained_model') # 학습된 모델 저장\n",
    "tokenizer.save_pretrained('./trained_model') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
